\chapter{Semantic parsing with bottom-up composition}\label{chp:parsing}

In the next three chapters,
we approach the task of answering questions on tables
%(Chapter~\ref{chp:tables})
with the \emph{grounded semantic parsing} framework.
As illustrated in Figure~\ref{fig:quick-example},
the input natural language question is parsed
into a \emph{logical form},
a formal compositional representation that roughly reflects
the semantics of the question.
The logical form can then be grounded (``executed'')
on the context (tables in our case)
to get concrete values representing the answer to the question.
The training data is used to train the model
for converting questions into logical forms,
while the execution process is deterministic.

Semantic parsing offers several benefits.
By representing the question as a compositional structure,
semantic parsing has been used to answer complex questions
that require multiple nesting operations.
This is in contrast to flat models (e.g., bag of words)
that are not compositional.
The logical form representation
also explicitly tells us how the answer is derived.
This is in contrast to methods that use 
continuous intermediate representations,
such as most deep neural models,
which are usually more difficult to interpret.

We first formalize the semantic parsing framework
in the context of our task.
We then describe the syntax of logical forms
and how they can be executed on the context tables.
Afterward, we explain our model that learns to parse
questions into logical forms via bottom-up generation.
Finally, we report our experimental results
and provide analysis of our model.

\section{Framework}

To recap, our task is to take a context table $w$
and a question $x$ about the table,
and then produce a list $y$ of values that answers the question.
The training data contains triples $(x,w,y)$
of questions, tables, and answers.

\paragraph{Logical forms.}
The main idea of semantic parsing
is to convert the question $x$
into a logical form $z$.
While the name of the framework suggests that
logical forms should encode the semantics of $x$,
we relax the notion of ``semantics'' in two ways:
\begin{enumerate}
\item The logical form $z$ only needs to
encode the necessary semantic information
for answering the question.
For instance, \todo{example}.
This in in contrast to a different line of work
that parses sentences into full semantic representations,
such as abstract meaning representation
\todo{cite}.
\item The logical form $z$ will be interpreted
in the context of the table $w$.
As such, it does not need to encode information
that would be redundant under the context.
For example, \todo{example}.
\end{enumerate}

There are many logical form languages proposed in
previous semantic parsing works.
Some works use representations inspired by linguistics
such as first order logic or \todo{find more and cite}.
Some use database query languages such as
SQL or SPARQL.
Some others design custom logical form languages
such as FuncQ or lambda DCS.
While our approach is agnostic to the language of logical forms,
we choose to use lambda DCS,
which we describe in more detail in Section~\ref{sec:logical-forms}.

\paragraph{Denotations.}
A logical form $z$ can be deterministically \emph{executed}
on the table $w$ to get a \emph{denotation}
$\deno{z}{w}$, which can be any arbitrary value.
\todo{Give a concrete example}
For a given target value $y$
(i.e., the correct answer to the question),
we call a logical form $z$ \emph{consistent} with $y$
if $\deno{z}{w} = y$,
and \emph{inconsistent} with $y$ if $\deno{z}{w} \neq y$.
Our goal is to predict a consistent logical form $z$
and then return its denotation.

\paragraph{Prediction.}
Given a table $w$ and a question $x$,
we predict a logical form $z$ and an answer $y$ using the framework
as shown in Figure~\ref{fig:prediction-framework}.
We first generate a set of candidate logical forms
$Z_x$ by parsing the question $x$ using the information
from the table $w$.
Each generated logical form $z \in Z_x$
is associated with its denotation $y = \deno{x}{w}$
and a score $s_\theta(z, x, w)$,
where $s_\theta$ is a scoring function
with trainable parameters $\theta$.
We define a distribution over the candidate logical forms as
\begin{equation}
p_\theta(z \mid x, w) \propto \exp\crab{s_\theta(z, x, w)}.
\end{equation}
Finally, we choose the logical form $z$ with the highest
model probability $p_\theta(z \mid x, w)$
and output its denotation $y = \deno{x}{w}$.

\paragraph{Training.}
Given training data $\{(x\i, w\i, y\i)\}_{i=1}^N$,
we use a gradient ascent method to optimize $\theta$ to maximize
one of the following objective functions:

\begin{enumerate}
\item \emph{Log-likelihood of the correct denotations.}
To compute the log-likelihood of a denotation $y$,
we marginalize
over the logical forms $z \in Z_{x}$
that executes to $y$:
\begin{equation}
p_\theta(y \mid x, w) =
\sum_{\substack{z \in Z_x \\ \deno{z}{w} = y}}
p_\theta(z \mid x, w).
\end{equation}
The objective function is
\begin{equation}
J_\Mr{LL}(\theta) = 
\frac{1}{N} \sum_{i=1}^N \log p_\theta(y\i\mid x\i, w\i)
- \Omega(\theta)
\end{equation}
where $\Omega$ is a regularization function.
Intuitively, a gradient update
will push up the scores of \emph{all}
consistent logical forms in $Z_x$
(i.e., the ones with the correct denotation),
and push down the scores of \emph{all}
inconsistent logical forms in $Z_x$.

\item \emph{Contrastive loss.}
From $Z_x$ generated from each question $x$,
we pick a logical form $z_+$
with the highest model probability among consistent
logical forms,
and another logical form $z_-$ with the highest
probability among inconsistent logical forms.
The objective function is defined as
\begin{equation}
J_\Mr{cnt}(\theta) =
\frac{1}{N} \sum_{i=1}^N \log
\frac{p_\theta(z\i_+\mid x\i, w\i)}{p_\theta(z\i_-\mid x\i, w\i)}
- \Omega(\theta)
\end{equation}
Intuitively, a gradient update
will push up the scores of \emph{one}
consistent logical forms
and push down the scores of \emph{one}
inconsistent logical forms.
Unlike the usual contrastive loss,
we always update the parameter
even when the probability of $z_+$
already exceeds that of $z_-$.
\todo{This should have led to a degenerated model}
\end{enumerate}

\section{Execution framework}
In this section, we describe our design of the execution framework,
including the language of logical forms
and how they are executed on the context table.

\subsection{Table representation}
We first explain how we represent the 
relationships among the rows,
columns, and cells of the table.
Inspired by the graph representation of knowledge bases,
we represent the table as a \emph{knowledge graph}
as illustrated in Figure~\ref{sec:knowledge-graph}.

\paragraph{Basic construction.}
To construct a knowledge graph,
we first convert each row into a \emph{row node}
(e.g., the first row becomes $r_1$),
and convert cells into \emph{cell nodes}
(e.g., the cell with text \nl{Athens}
becomes a node \T{Athens}).
Then, we convert each column into directed \emph{column edges}
from the row nodes to the corresponding entity nodes
of that column,
and label the edges with the column header
(e.g., we construct an edge with label \T{City}
from $r_1$ to \T{Athens}).
Note that columns are automatically renamed
to have unique texts if necessary.

\paragraph{Graph augmentation.}
One benefit of the graph representation
is that we can freely augment the graph with 
additional information that helps us parse the questions.

The first type of augmentation we employ is
\emph{normalization nodes}.
Some cell strings (e.g., \nl{1900})
can be interpreted as a number, a date, or a proper name
depending on the context,
while some other strings (e.g., \nl{200 km} and \nl{21-14})
have multiple parts.
Instead of committing to one normalization scheme,
we introduce normalization nodes for the possible ways
to interpret the cell strings,
and use special edges to link cell nodes to normalization nodes.
We use the following types of normalization:

\begin{itemize}
\item \todo{Fill in all types of normalization from the code}
\end{itemize}

The second type of augmentation are row-specific edges.
Questions about tables usually involve reasoning about the
order of the rows. For instance,
\nl{What is the next \dots?} or \nl{Who came before \dots?}
require looking at adjacent rows,
while \nl{Who is the last \dots?} requires looking
at the last row among a group of rows.
To help answer these types of questions,
we augment each row node with an edge labeled \T{Next}
pointing to the next row node,
and an edge labeled \T{Index} pointing to the row index number
(starting from 1).

\subsection{Logical forms}\label{sec:logical-forms}
As the language of our logical forms,
we use \emph{lambda dependency-based
compositional semantics}, or \emph{lambda DCS}
\cite{Liang2013LambdaDC}.
The language was originally designed for semantic parsing
on large knowledge graphs
\cite{Berant2013SemanticPO}.
As the context table can be converted into a knowledge graph
following the previous section,
the lambda DCS formalism naturally transfers to our setting.

We now describe the syntax and semantics of lambda DCS constructs.

\todo{Copy from the Neo Lambda DCS doc}
\todo{Make a reference table}

\section{Parsing framework}

Given a knowledge graph $w$,\footnote{
We overload the variable $w$ for both the table and
its knowledge graph representation.
Clarification will be made when necessary.}
we want to parse the input question $x$
and generate a set $Z_x$ of candidate logical forms.
This section describes a bottom-up semantic parser
that produce the logical forms and their scores.

\subsection{Deduction rules}
The space of possible logical forms given the knowledge graph $w$
and the question $x$ is defined recursively
by a set of \emph{deduction rules},
which dictate how logical forms
can be constructed either from the inputs
or from smaller logical forms.
Informally,
our parser
maintains a set of logical forms,
and then repeatedly applies deduction rules
to construct larger logical forms
from smaller ones in the set.
Logical forms that are ``well-formed''
are finally compiled into
the set $Z_x$ of candidate logical forms.
The parsing algorithm will be described more formally
in Section~\ref{sec:floating-parser}.

Deduction rules are divided into two types: base rules
and compositional rules.

\paragraph{Base rules.}
A base rule creates logical forms based on the input
question $x$ and knowledge graph $w$.
Each base rule follows one of the following templates:
\begin{align}
\C{TokenSpan}[s] &\to c[f(s)] \label{eqn:rule-b1} \\
\varnothing &\to c[f()] \label{eqn:rule-b2}
\end{align}

A rule of Template~\ref{eqn:rule-b1} takes
a token span from the question $x$
and applies the function $f$,
which generates a set of logical forms.
The resulting logical forms will be associated with
a \emph{category} $c$, which is used to enforce type consistency.
For instance, let $\mathrm{fuzzymatch}$ be a function
that takes a string $s$ (e.g., \nl{German})
and returns cell nodes whose cell content strings
approximately match $s$ (e.g., \T{Germany}).
Then the deduction rule
\begin{equation}
\C{TokenSpan}[s] \to \C{Set}[\mathrm{fuzzymatch}(s)]
\end{equation}
can construct logical forms of category \C{Set}
by applying the function $\mathrm{fuzzymatch}$
on some token span $s$ of $x$.

A rule of Template~\ref{eqn:rule-b2} works similarly,
but does not require any string from the question $x$.
Apart from generating input-independent logical forms
(e.g., \T{>=}\, and \T{allRows}),
this template is also useful when the logical form
is difficult to infer deterministically from the question.
For example, in most questions,
the relevant column names are indirectly mentioned
or implicitly implied
(e.g., the question \runningEx
in \ref{fig:running}
indirectly mentions the columns \T{Year} and \T{Position}).
We can use the following deduction rule
to generate all columns from the table:
\begin{equation}
\varnothing \to \C{Rel}[\mathrm{graphEdges}()],
\end{equation}
where the function $\mathrm{graphEdges}$ returns all unique edges
from the knowledge graph $w$. (The rule will also generate
augmented edges such as \T{Index}, \T{Next}, and \T{Num}.)
The scoring model will later decide which columns
are relevant to the question.

\paragraph{Compositional rules.}
A compositional rule constructs larger logical forms
from smaller ones.
Each compositional rule follows one of the following templates:
\begin{align}
c_1[z_1] &\to c[g(z_1)] \label{eqn:rule-c1} \\
c_1[z_1] + c_2[z_2] &\to c[g(z_1, z_2)] \label{eqn:rule-c2}
\end{align}
A rule of Template~\ref{eqn:rule-c1}
takes a child logical form $z_1$ of category $c$,
and construct a new logical form $g(z_1)$ of category $c$.
For instance, if $g(z_1) = \T{count}(z_1)$,
we can construct the logical form
$\T{count}(\xHas{Position}.\T{1st})$
from an existing logical form $\xHas{Position}.\T{1st}$.
A rule of Template~\ref{eqn:rule-c2}
operates similarly but takes two children logical forms.

\paragraph{List of deduction rules.}
Table~\ref{tab:deduction-rules}
details all deduction rules used in our parser.
The deduction rules were designed to be closely mimic
the compositional syntax of lambda DCS.
However, as some lambda DCS constructs are very generic
and can generate many nonsensical logical forms
(e.g., the subtraction operator can subtract
any two arbitrary numbers),
%we trade-off generality for control
some operators are restricted to be applied
only in certain contexts
(e.g., only allow subtracting numbers from two cells
from the same column).
This trade-off prevents us from
answering some small number of sophisticated questions,
but we found it necessary for making the space
of generated logical forms manageable.

From the table,
we can see that many deduction rules
construct logical forms
without referencing the question.
These include the base rules of the form
$\varnothing \to c[f()]$
and various compositional rules
that generate operators
(e.g., \T{argmax} can be generated even when
the question does not have any word that expresses superlative).
This is intentional for two reasons:
\begin{itemize}
\item Many logical form predicates do not explicitly align
to any token from the question.
\item Even when the alignment exist, we want to learn
such an alignment from the data.
This will be achieved by the features in the scoring module
that relate logical form predicates to the tokens in the question.
\end{itemize}

\subsection{Floating parser}\label{sec:floating-parser}
To parse the question $x$ based on the deduction rules,
we propose a new parser named \emph{floating parser}
that can construct logical forms in a flexible order.

\paragraph{Chart parser.}
To understand the motivation behind the floating parser,
let us first consider a more common bottom-up parsing algorithm:
the CKY algorithm for chart parsing.

% The CKY algorithm for chart parsing
% is used for syntactic parsing \todo{cite}
% and sometimes \todo{some other things?}.
% We will apply the CKY algorithm on our semantic parsing task.
Given an input sentence $x$ with tokens $x_1, \dots, x_n$,
the algorithm constructs and stores partial parses
with category $c$
of the token span $x_{i:j} := (x_i, \dots, x_{j-1})$
in a \emph{cell} labeled $(c, i, j)$.
Being a dynamic programming algorithm,
the CKY algorithm populates the cells in the increasing order
of their span lengths $j - i$.
To construct a parse for the cell $(c, i, j)$,
we have the following choices:
\begin{itemize}
\item Apply a base rule of the form
\begin{equation*}
\C{TokenSpan}[s] \to c[f(s)]
\end{equation*}
on the token span $s = x_{i:j}$ to get logical forms $z \in f(s)$.
\item Apply a compositional rule of the form
\begin{equation*}
c_1[z_1] + c_2[z_2] \to c[g(z_1, z_2)]
\label{eqn:rule-c2-again}
\end{equation*}
on $z_1$ from cell $(c_1, i, k)$ and
$z_2$ from cell $(c_2, k, j)$ (for some $k \in \set{i,\dots,j-1}$)
to get a logical form $z = g(z_1, z_2)$
\item Apply a compositional rule of the form
\begin{equation*}
c_1[z_1] \to c[g(z_1)]
\end{equation*}
on another logical form $z_1$ from the same cell $(c, i, j)$
to get $z = g(z_1)$.
To avoid an infinite loop,
one can either design the deduction rules
to not have loops,
or use heuristics to detect and stop loops.
\end{itemize}

In any case, the parse is an abstract object containing
the constructed logical form $z$, plus any other metadata necessary
to score the logical form (e.g., when applying
Rule~\ref{eqn:rule-c2-again}, we can store the child parses
where $z_1$ and $z_2$ come from).
To restrict the search space to a reasonable size,
\emph{beam search} is usually employed:
the populated cells are pruned down to some
fixed number of parses that have the highest scores.
The parses in cell $(\C{ROOT}, 0, n)$
form the set $Z_x$ of final logical forms.

\paragraph{Challenges.}
Chart parsing founds success in syntactic parsing,
where all words end up participating in the parse,
and the parse forms a proper tree with no reordering.
In contrast, for our semantic parsing task,
the chart parser is restrictive for several reasons:
\begin{itemize}
\item
While each parse in chart parsing belongs
to some token span $x_{i:j}$,
some semantic predicates do not naturally align
to a token.
Consider the following question on a table about
Olympic games:
\todo{Find a more convincing example}
\begin{center}
\nl{Greece held its last Summer Olympics in which year?} \\
$\xOf{Date}.\xOf{Year}.\T{argmax}(\xHas{Country}.\T{Greece}, \lambda r.\xOf{Index}.r)$
\end{center}
While the cell node \T{Greece} can be generated from
the word \nl{Greece}, some logical form predicates
such as \T{Country} cannot be aligned to a token span.
We could potentially learn to generate the whole
$\xHas{Country}.\T{Greece}$ from \nl{Greece},
but this requires writing more custom deduction rules.
\item
Conversely, some token does not align with
a semantic predicate.
In the example above, \nl{Summer} and \nl{Olympics}
do not manifest in the logical form.
\item
Finally, the order of question tokens
and the logical form predicates
may not align well.
In the example above,
the word \nl{year} comes last,
but the predicate \T{Year} comes first.
\end{itemize}

While there are ways to modify chart parsing to handle the
challenges above, we opt for a more general solution
by proposing the \emph{floating parser},
which composes logical forms more flexibly.

\paragraph{Floating parser.}
The floating parser does not require logical form predicate to
be constructed from utterance tokens.
We replace the cells $(c, i, j)$,
with \emph{floating cells} $(c, s)$,
which will store logical forms
of category $c$ that have size $s$.
The size is measured as the number of compositional rules applied
during the construction of the logical form,
and not the number of logical form predicates.

The deduction rules now operate as follows:
\begin{itemize}
\item A base rule of the form
\begin{equation*}
\C{TokenSpan}[s] \to c[f(s)]
\end{equation*}
constructs $z \in f(s)$ in cell $(c, 0)$.
\item A base rule of the form
\begin{equation*}
\varnothing \to c[f()]
\end{equation*}
constructs $z \in f()$ in cell $(c,0)$.
This allows logical form predicates to be generated
out of thin air.
\item A compositional rule of the form
\begin{equation*}
c_1[z_1] + c_2[z_2] \to c[g(z_1, z_2)]
\label{eqn:rule-c2-again}
\end{equation*}
takes $z_1$ from cell $(c_1, s_1)$ and
$z_2$ from cell $(c_2, s_2)$
to construct $z = g(z_1, z_2)$
in cell $(c, s_1 + s_2 + 1)$.
\item A compositional rule of the form
\begin{equation*}
c_1[z_1] \to c[g(z_1)]
\end{equation*}
takes $z_1$ from cell $(c_1, s_1)$ 
to construct $z = g(z_1)$ in cell $(c, s_1 + 1)$.
\end{itemize}

Figure~\ref{fig:floating-parse-ex} shows
an example parse generated by our floating parser.
Finally,
the parses in cell $(\C{ROOT}, s)$ for all sizes $s$
form the set $Z_x$ of final logical forms.

\paragraph{Pruning.}

The floating parser is very flexible:
it can skip tokens,
generate tokens out of thin air,
and combine logical forms in any order.
This flexibility might seem too unconstrained,
but we can use several techniques to prevent
bad logical forms from being constructed:

\begin{itemize}
\item \textbf{Denotation-based pruning.}
Constructed logical forms can be executed
on the knowledge graph $w$ to get denotations.
We prune logical forms that execute to an empty set
(e.g., $\xHas{Year}.\xHas{Num}.\C{1}$).
While it is tempting to prune logical forms that do not execute,
care must be taken since some constructed logical forms
are partial and are meant to be used as an argument
of a larger logical form
(e.g., logical forms of category \C{ValueFn}
are meant to be used in superlative logical forms).

\item \textbf{Heuristic pruning.}
Some logical forms can be properly executed,
but are unlikely to be relevant for answering the questions.
For instance, a union of objects from different columns
(e.g., $\T{5th} \sqcup \T{Germany}$)
are less likely to be reflecting what the question asks.
We use several heuristics listed in Table~\ref{tab:heuristics}
to prune logical forms.
Note that some heuristics could potentially prevent us
from answering some questions
(e.g., preventing double \T{Next}
could prevent us from answering
\nl{What comes before the person before \dots?}).
However, these questions are rare enough,
and pruning these patterns would prune a large portion of the
search space, which increases speed and prevents overfitting
to bad patterns.

\item \textbf{Score-based pruning.}
Even with the pruning strategies above,
the set of possible logical forms in each set
might still be large.
To control the search space,
we employ beam search by scoring the logical forms
and only keep the $B = 50$
highest-scoring logical forms in each cell.

\end{itemize}

\subsection{Scoring}
Each logical form $z$ is associated with a score $s_\theta(z, x, w)$.
We adopt a linear model with features that capture the relationship
between the question $x$ and the logical form.
Concretely,
\begin{equation}
s_\theta(z, x, w) := \theta^\top \phi(z, x, w)
\end{equation}
where $\phi(z, x, w)$ is a feature vector.
Table~\ref{tab:features-ex}
shows example features from each feature type
described below:
% Most features described below are indicator features
% of the form $(f_\Mr{x}(x), f_\Mr{z}(z))$,
% which relates tokens in $x$ with predicates in $z$,
% or $(f_\Mr{x}(x), f_\Mr{y}{y})$,
% which relates the question with the denotation
% $y = \deno{z}{w}$.

\begin{itemize}

\item \textbf{phrase-predicate}:
For each n-gram $p_\Mr{x}$ from the question $x$
(up to 3-grams)
and a predicate $p_\Mr{z}$ from $z$,
we define a lexicalized
indicator feature
$\Mr{pp}(p_\Mr{x}, p_\Mr{z})$.
We also define an unlexicalized feature
$\Mr{pp}(=)$
when the $p_\Mr{x}$ matches the original string of $p_\Mr{z}$.

\item \textbf{missing-predicate}:
We define an unlexicalized indicator feature
when there are entities or relations mentioned in $x$
that are not present in $z$.

\item \textbf{denotation}:
From the denotation $y = \deno{z}{w}$,
we use its size (i.e., number of elements in the set $y$)
and its type to define unlexicalized features.
The type can be a primitive type (e.g., \Sc{Number}, \Sc{Date})
or the column containing the values in $y$
in case those values are cells.

\item \textbf{phrase-denotation}:
For each n-gram $p_\Mr{x}$ from the question $x$,
we define a lexicalized feature $\Mr{pd}(p_\Mr{x}, p_\Mr{y})$
where $p_\Mr{y}$ is the type of $y$.
Like the phrase-predicate features,
we also define an unlexicalized feature
$\Mr{pd}(=)$
when the type $p_\Mr{y}$ is a column whose string matches $p_\Mr{x}$.

\item \textbf{headword-denotation}:
From the part-of-speech tags of the question $x$,
we deterministically identify the question word $q_\Mr{x}$
(e.g., \emph{who}, \emph{when}, \emph{what})
and the head word $h_\Mr{x}$
(the first noun after the question word).
We then define lexicalized features $\Mr{hd}(q_\Mr{x}, p_\Mr{y})$
and $\Mr{hd}(h_\Mr{x}, p_\Mr{y})$,
where $p_\Mr{y}$ is the type of $y$.
\end{itemize}

\section{Experiments}
We develop our model on three 80:20 splits
of the training portion of the \wtq dataset (14,152 examples),
where we report the average of three evaluation scores.
For the final evaluation,
we train on the training portion and 
test on the ``unseen'' test portion
(4,345 examples).
\todo{Rerun with the ``seen'' test portion?}

\subsection{Main evaluation}
The main evaluation metric is \emph{accuracy}:
the fraction of test examples where
the parser predicts the correct denotation
(i.e., the highest-ranking logical form is consistent
with the correct answer).
We also report the \emph{oracle} score:
the fraction of test examples where
the at least one of the logical forms in the
final candidate list $Z_x$ is consistent.

\paragraph{Baselines.}
We compare our systems to two baselines:
\begin{itemize}

\item \textbf{Information retrieval baseline (IR)}:
The IR baseline selects a cell $y$
among the table cells by applying a log-linear model
over the cells.
The features are conjunctions of the phrases of $x$
and the properties of $y$,
which covers all features of our parser
that do not depend on the logical form.

\item \textbf{WebQuestions baseline (WQ)}:
We restrict the logical form operators to the ones
present in previous work on the WebQuestions dataset
\cite{Berant2013SemanticPO}.
This includes the join operation and the \T{count} aggregate.
\end{itemize}

\paragraph{Results.}
Table~\ref{tab:c4-results}
shows the accuracy and oracle scores
of the floating parser and the baseline systems.

In the following sections,
we use the development split of the training data
to analyze various aspects of the \wtq dataset
and the floating parser.

\subsection{Error analysis}

The error on the development data can be divided into the
following categories:

\paragraph{Unhandled question types (\qqq\%).}
Due to our choices of knowledge graph representation
and logical form syntax,
some questions in the dataset cannot be answered
with logical forms.
The majority of them are:

\todo{Give reason for each}
\begin{itemize}
\item Questions with incorrect annotations.
\item Yes-no questions.
(e.g., \todo{example})
\item Questions with the word \nl{same} or similar.
(e.g., \todo{example})
\item Questions with the word \nl{consecutive} or similar.
(e.g., \todo{example})
\end{itemize}

\paragraph{Failure to match cells (\qqq\%).}
The cell predicates and atomic values in the logical forms
are created from two basic rules:
\begin{align*}
\C{TokenSpan}[s] & \to \C{Entity}[\Mr{fuzzymatch}(s)] \\
\C{TokenSpan}[s] & \to \C{Atomic}[\Mr{value}(s)] \\
\end{align*}

While $\Mr{fuzzymatch}(s)$ uses approximate string matching
to identify the cell from the token $s$,
sometimes the question uses a synonym
\todo{example}
or does not mention the cell at all
\todo{example}.
While it is possible to generate cell predicates
with a rule of the form $\varnothing \to \C{Entity}[f(s)]$
like how the column predicates are generated,
such a rule would explode the search space
as most table has a large number of cells
(\qqq on average).

Likewise, the $\Mr{value}(s)$ function that interpret
the string $s$ as numbers or dates is not perfect.
\todo{example}.

\paragraph{Complex cell content (\qqq \%).}
While we use normalization edges to handle different
interpretation of the cell string,
we observe several types of strings we cannot handle.
Some of these include times (e.g., \todo{example})
and multi-part strings (e.g., \todo{example}).

\paragraph{Ranking errors (\qqq \%).}
Finally, we have ranking errors
where the consistent logical form is scored lower
than the top logical form.
The most common cause is rare column names
that are not mentioned directly in the question
(e.g., \todo{example}).
A model that incorporates continuous word representations
could potentially reduce this type of errors.

\subsection{Ablation analysis}

\paragraph{Effect of features.}
Table~\ref{tab:c4-ablation}(a)
shows the development accuracy when a subset of features
are ablated.
The most important features are the 
lexicalized phrase-predicate features,
which learn the direct association between
words from the questions and logical form predicates
(e.g., associating \nl{last} to \T{argmax},
or associating \nl{who} with the column \T{Name}).

Table~\ref{tab:c4-best-features}
shows the features whose parameter weight have the highest magnitude.
We observe that the model indeed learns to associate
question phrases with either built-in logical predicates
or common column names.
It also learns some biases in the dataset;
for instance, the feature $\Mr{d}(\Mr{size} = 1)$
indicates that the answer tends to be a single value
rather than a set of multiple values.

\paragraph{Associating operators with tokens.}
In our floating parser,
built-in edges (e.g., \T{Index}, \T{Next}, \T{Num})
and logical operators (e.g., \T{count}, \T{argmax}, \T{sub})
are not generated based on the question tokens,
but the model ends up learning the association
from these predicates to the tokens.
As an experiment,
we consider an alternative approach where
these predicates are explicitly generated based on
some set of ``trigger'' phrases.
Based on the training data,
we manually specify the trigger phrases
for each logical predicate,
as listed in Table~\ref{tab:trigger-phrases}.
We then change the deduction rules so that
the predicates can be constructed
only when one of the phrases is present in the question.

As an explicit prior,
the trigger words help decrease the number of
generated logical forms
(before pruning to beam size)
\todo{statistics}.
However, the result in Table~\ref{tab:c4-ablation}(b)
shows that trigger words
do not significantly affect the accuracy,
suggesting that our floating parser
can successfully learn the association
between phrases and logical operators
without requiring a lexicon.

As a side note, the oracle score of the trigger words setting
is much lower than that of our floating parser.
This is because in our parser,
incorrect interpretations of the question
(which can be generated due to the flexible generation)
ends up executing to the correct answer.
These \emph{spurious} logical forms will be explored
in more detail in %Section~\ref{sec:c4-spurious} and
the next chapter.

\paragraph{Effect of beam size.}
Figure~\ref{fig:c4-beam}
shows the accuracy and oracle scores
as the beam size changes.
A lower beam size increases efficiency
and prevents bad logical forms from clogging up the beam,
which can hurt the accuracy.

\subsection{Dataset analysis}
From Table~\ref{tab:c4-results},
our parser successfully finds a logical form
consistent with the correct denotations in
\qqq of the development examples.
We use these consistent logical forms
to examine the complexity of our dataset.

\paragraph{Logical form coverage.}
The \wtq dataset contains various types of reasoning
to answer the questions.
Table~\ref{tab:c4-ablation}(c) shows the drop in accuracy
and oracle scores
when only a subset of deduction rules are allowed.
The \emph{join only} subset corresponds to table lookup,
while the \emph{join + count} subset covers the same scope
of logical forms as the previous work on the WebQuestions dataset
\cite{Berant2013SemanticPO}.
Finally, the \emph{join + count + superlative} subset
roughly corresponds to the coverage of the GeoQuery dataset.

\todo{Forward reference to the next chapter.}

\paragraph{Compositionality.}
The histogram in Figure~\ref{fig:c4-lf-size}
tracks the size of the logical forms
(as the number of compositional deduction rules)
that are consistent with the correct answer.
If there are consistent multiple logical forms in
the candidate set $Z_x$, we choose the shortest one.
The histogram shows that a significant number of logical forms
have non-trivial sizes.

\paragraph{Spurious logical forms.}
One aim of the \wtq dataset is to expand the size of the domain
(breadth) and the complexity of the question (depth).
With more options for logical predicates
and more compositionality,
there are more ways to construct logical forms
that execute to a target value
without following what the question asks.
For instance, if the target is the number 2,
then there are many ways to construct a set of size 2
and then apply \T{count} to get the answer.

These \emph{spurious} logical forms can hurt us
during training if the model decides to upweight the features
corresponding to spurious logical forms.
With enough examples, the model could
use the aggregate statistics to learn the correct features,
but it would be better if we can remove
spurious logical forms in advance so that
the model can be trained more easily.
In the next chapter,
we will look at algorithms that can (partially)
detect and eliminate spurious logical forms.

\section{Related works}

\todo{Write this}
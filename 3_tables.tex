\chapter{Compositional question answering
on Web tables}\label{chp:tables}

\todo{Talk about compositionality}

For the rest of this thesis,
we will consider the task of answering complex questions
based on a given semi-structured Web table.

\section{Task description}
Our task is as follows:
given a context table $w$ (``world'')
and a question $x$ about the table,
output a list of values $y$
that answers the question according to the table.
Example inputs and outputs are shown in Figure~\ref{fig:running}.

At training time, the system has access to training data
$\{(x\i, w\i, y\i)\}_{i=1}^N$ of questions, tables, and answers.
In particular, the training data does not contain
\emph{how} the answers were derived from the questions.

To test that the system generalizes to unseen information,
we ensure that the tables in training and test sets
do not overlap.

\todo{Talk more about how the questions are complex}

\section{Dataset}

As a benchmark for the task,
we created a new dataset, \wtq, of question-answer pairs
on tables from Wikipedia.

%\subsection{Data collection}

The data collection process is as follows.
From the Wikipedia dump,
we randomly select data tables with at least
8 rows and 5 columns.
We give preference to tables with a high fraction of
numerical data, as more interesting questions
with comparison and calculation can be asked about the table.

After selecting the tables,
we created two tasks on Amazon Mechanical Turk,
a crowdsourcing service.
In the first task, we ask the workers to write four
trivia questions about the displayed table.
For each question,
we randomly gave the worker one of the 36 generic prompts
in Table~\ref{tab:turk-prompt}
to encourage more complex questions.
The worker can elect to change the prompt
if it is impossible or impractical to follow.

In the second task, we ask workers to answer
the questions from the first task on the given table.
To aid the workers, we provided shortcuts
for copying answers from the table,
counting the number of selected cells,
and computing statistics of the numbers in selected cells.
We only keep the answers that were agreed upon
by at least two workers.

The final dataset contains 22,033 examples
on 2,108 tables of various topics.
We set aside 20\% of the tables and their associated questions
as test set and only develop on the remaining examples.
We performed simple preprocessing on the tables:
we remove all non-textual contents
(e.g., images and hyperlink targets),
and if there is a merged cell spanning many rows or columns,
we unmerge it and copy the content into each unmerged cell.
In later releases of the dataset,
we also cleaned up encoding issues and
enforced formatting consistencies among the answers.

\section{Dataset analysis}

With respect to contemporary datasets,
the \wtq dataset was designed to cover a wider variety of domains
(``breadth'') and more complex questions (``depth'').
This section analyzes various aspects of the dataset
and compare them against related datasets.

\subsection{Breadth: diversity of data schema}

One measure of the variety of domains
is the diversity of the data schema.
For tabular data, this can be roughly measured
by the number of different column types
indicated by the column header.
The \wtq dataset contains 2,108 tables.
Among them, there are 13,396 columns
and 3,929 unique column headers.
The diversity of column headers
is much higher than traditional semantic parsing datasets,
such as Geoquery \cite{Zelle1996LearningTP}
and ATIS \cite{Price1990EvaluationOS},
which use small and fixed databases.

Previous work has also considered using
large knowledge bases as the context
for semantic parsing.
Examples include
\textsc{Free917} \cite{Cai2013LargescaleSP},
\textsc{WebQuestions} \cite{Berant2013SemanticPO},
and \textsc{SimpleQuestions} \cite{Bordes2015LargescaleSQ}.
While large knowledge bases
such as Freebase \cite{Bollacker2008FreebaseAC}
can cover a large number of relations
among entities,
they usually have a predefined schema.
In contrast, tables from the Web can have
any arbitrary schema depending on the authors
of the tables.

\subsection{Breadth: knowledge coverage}

Knowledge bases are difficult to build
and are usually incomplete.
We sample 50 examples from the \wtq dataset
and tried to answer them manually by
querying Freebase.
Even though Freebase contains some information
extracted from Wikipedia,
we can answer only 20\% of the questions,
indicating that the dataset covers
a broader range of knowledge domains than Freebase.

\subsection{Depth: compositionality}

One selling point of traditional semantic parsing datasets
such as Geoquery \cite{Zelle1996LearningTP}
is the complexity of the questions.
One extreme example is the question
\nl{What states border states that border states that border states that border Texas?},
which requires four levels of reasoning.

While the \wtq dataset is not as compositional,
\todo{give examples}

In Section \todo{ref}, we will return to analyze
compositionality
by looking at the complexity of the logical form
representations of the questions.

\subsection{Depth: types of operations}

The questions in the \wtq dataset require
a diverse set of operations to answer the questions.
We manually classified 200 examples based on the
types of operations required to answer the questions.
The statistics in Table \todo{ref} shows that
while a few questions only require a simple operation
such as table lookup or counting, the majority of the questions
demands more complex operations.